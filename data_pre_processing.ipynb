{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data pre_processing",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-abQNHBM9bC6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import package and data"
      ]
    },
    {
      "metadata": {
        "id": "A2UV-RNA3nsd",
        "colab_type": "code",
        "outputId": "63035b0f-f22a-40c9-e1a9-1a667721147b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# # This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. click the link below and authorize it in google drive \n",
        "# 2. paste the link in tq he white box7lñ.ijkughtydesw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cLCanu4jx1cd",
        "colab_type": "code",
        "outputId": "b339128b-c1ff-42ad-b2fa-3206f4356dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "h2x8zS2r_v69",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Merge image with label and sample"
      ]
    },
    {
      "metadata": {
        "id": "9emA_jzXDv-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_name(time=datetime.datetime(2018, 11, 2, 3, 21, 2)):\n",
        "\n",
        "# datetime.datetime.fromtimestamp(os.path.getctime(os.getcwd()))>datetime.datetime(2018, 10, 25, 3, 21, 2)\n",
        "  # read img list csv, this file contains the label of images\n",
        "  os.chdir(\"/content/drive/My Drive/Networkly/Emotion/Datasets/AffectNet_Database/Manually_Annotated_file_lists\")\n",
        "  print('read_file')\n",
        "  data = pd.read_csv('training.csv')\n",
        "  count_=pd.DataFrame(data.subDirectory_filePath.str.count('/'))\n",
        "  data['main'],data['pic_name']=data.subDirectory_filePath.str.split('/').str\n",
        "\n",
        "  os.chdir(\"/content/drive/My Drive/Networkly/Emotion/Datasets/AffectNet_Database/Manually_Annotated_compressed/Manually_Annotated_Images\")\n",
        "\n",
        "  # count all the images in this dir\n",
        "  subdirs=os.listdir(os.getcwd())\n",
        "\n",
        "  r = re.compile(\"[0-9]+$\")\n",
        "  subdirs = list(filter(r.match, subdirs)) # Read Not\n",
        "\n",
        "  print('getting file name')\n",
        "  cwd=os.getcwd()\n",
        "  a=[]\n",
        "  b=[]\n",
        "  for subdir in tqdm(subdirs):\n",
        "    path=os.path.join(os.getcwd(),subdir)\n",
        "    for file in os.listdir(path):\n",
        "      file_path=os.path.join(path,file)\n",
        "      c_time=datetime.datetime.fromtimestamp(os.path.getctime(file_path))\n",
        "      if c_time>time:\n",
        "        a.append(file) #new file \n",
        "      else:\n",
        "        b.append(file) #old file\n",
        "  print('length of existing files is {}'.format(len(b)))\n",
        "  print('length of updated files is {}'.format(len(a)))\n",
        "  print('')\n",
        "  a=pd.DataFrame(a)\n",
        "  # a=pd.DataFrame(b)\n",
        "  a.columns=['pic_name']\n",
        "\n",
        "  # expression dsc\n",
        "  data_dict={0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger',\n",
        "  7: 'Contempt', 8: 'None', 9: 'Uncertain', 10: 'No-Face'}\n",
        "\n",
        "  # a['name'],a[1]=a[0].str.split('.',columns = ['name','row']).str\n",
        "\n",
        "  # join it with dsc data and count the frequency of each expressions\n",
        "  merged=a.merge(data,how='left',on='pic_name')\n",
        "  count_=merged.groupby('expression',as_index=False).count()\n",
        "\n",
        "  # get value from dict\n",
        "  count_['emotion_dsc']=count_[count_.expression.notna()].expression.apply(lambda x:data_dict.get(x))\n",
        "  count_.head()\n",
        "  print('expression count')\n",
        "  print(count_[['emotion_dsc','pic_name']])\n",
        "  return merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRAsU5u1E2ub",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function \n",
        "def get_sample(major_case_smpl_size=1500):\n",
        "  # collect all the rare cases\n",
        "  rare_case=merged[(merged.main.notna())&(merged.expression.isin([7,5,4,3,6,2]))]\n",
        "\n",
        "  # sample more frequent cases, each 20000\n",
        "  frequent_case=merged[(merged.main.notna())&(merged.expression.isin([0,1]))]\n",
        "  grouped = frequent_case.groupby('expression')\n",
        "  frequent_case=grouped.apply(lambda x: x.sample(major_case_smpl_size,random_state=5))\n",
        "  # combine \n",
        "  df_sample=pd.concat([rare_case,frequent_case],axis=0)\n",
        "  print('sample size is {}'.format(df_sample.shape[0]))\n",
        "  return df_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xYzAM1QgTNuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# function used to convert imgs\n",
        "def read_img(call_back=0):\n",
        "  \n",
        "  # annotations.head()\n",
        "  os.chdir(\"/content/drive/My Drive/Networkly/Emotion/Datasets/AffectNet_Database/Manually_Annotated_compressed/Manually_Annotated_Images\")\n",
        "\n",
        "  paths = df_sample[\"subDirectory_filePath\"]\n",
        "  labels = df_sample[\"expression\"]\n",
        "  image = []\n",
        "  value = []\n",
        "  \n",
        "  print('start converting')\n",
        "  for i in tqdm(range(len(labels))[call_back:]):\n",
        "    if os.path.exists(str(paths[i])):\n",
        "      tobegray = cv2.imread(paths[i])\n",
        "      gray=cv2.cvtColor(tobegray, cv2.COLOR_BGR2GRAY)\n",
        "      resized=cv2.resize(gray, (128,128))\n",
        "      image.append(resized)\n",
        "      value.append(labels[i])\n",
        "      if i%8000==0 or i==len(labels)-1:\n",
        "        np.save(str(i)+'_11_4_image.npy', image)\n",
        "        np.save(str(i)+'_11_4_label.npy', value)\n",
        "        print('file  {}  is saved'.format(str(i)+'_11_04_image.npy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fJ94bcvA5LF",
        "colab_type": "code",
        "outputId": "09dcf0a9-3ff0-4608-fcd7-1cee31f8b307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# merged=get_file_name()\n",
        "df_sample=get_sample(major_case_smpl_size=11500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-72662273bb14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_case_smpl_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-0b24325a45f3>\u001b[0m in \u001b[0;36mget_sample\u001b[0;34m(major_case_smpl_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_case_smpl_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# collect all the rare cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mrare_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# sample more frequent cases, each 20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rJfgrChaSBiX",
        "colab_type": "code",
        "outputId": "b154f587-f95e-40c5-a376-d44712226d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "read_img(call_back=32000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/36647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start converting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/36647 [00:00<3:20:29,  3.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file  0_11_04_image.npy  is saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 8001/36647 [35:38<3:37:33,  2.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file  8000_11_04_image.npy  is saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 16001/36647 [1:11:40<3:18:02,  1.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file  16000_11_04_image.npy  is saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 17138/36647 [1:17:19<2:22:44,  2.28it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kZf7qWz9BqLq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "glob.glob('*.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFgXtcPe2zX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/Networkly/Emotion/Datasets/AffectNet_Database/Manually_Annotated_compressed/Manually_Annotated_Images\")\n",
        "np.load('800010_27_label.npy').shape,np.load('48000_10_27_label.npy').shape,np.load('56884_10_27_label.npy').shape\n",
        "\n",
        "a=np.load('800010_27_image.npy')\n",
        "b=np.load('48000_10_27_image.npy')\n",
        "c=np.load('56884_10_27_image.npy')\n",
        "\n",
        "a1=np.load('800010_27_label.npy')\n",
        "b1=np.load('48000_10_27_label.npy')\n",
        "c1=np.load('56884_10_27_label.npy')\n",
        "\n",
        "images=np.concatenate((a,b,c),axis=0)\n",
        "labels=np.concatenate((a1,b1,c1),axis=0)\n",
        "\n",
        "# np.save('11_1_final_images',images)\n",
        "# np.save('11_1_final_labels',labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwZh2W8jNrqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a=np.load('10_28_images_all.npy')\n",
        "b=np.load('10_28_label_all.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IXBMRlgIOFj-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "images=np.concatenate((a,images),axis=0)\n",
        "labels=np.concatenate((b,labels),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kg6h8zdCjX2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-feioVQOUX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('11_1_final_images_combine_all_history',images)\n",
        "np.save('11_1_final_labels_combine_all_history',labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9zMqgCO3IBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# extract zar file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xq-NVewd67Vs",
        "colab_type": "code",
        "outputId": "55f51201-3f9b-4914-9e36-013574e697eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Manually_Annotated_Images',\n",
              " 'part2',\n",
              " 'Manually_Annotated.part05.rar',\n",
              " 'Manually_Annotated.part06.rar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "kbcW7LmF3oHr",
        "colab_type": "code",
        "outputId": "ab2cedb3-10c9-4a4e-c158-7681c5997efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Networkly/Emotion/Datasets/AffectNet_Database/Manually_Annotated_compressed')\n",
        "!apt-get install unrar\n",
        "!unrar x Manually_Annotated.part06.rar\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  unrar\n",
            "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
            "Need to get 129 kB of archives.\n",
            "After this operation, 322 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 unrar amd64 1:5.5.8-1 [129 kB]\n",
            "Fetched 129 kB in 1s (168 kB/s)\n",
            "Selecting previously unselected package unrar.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../unrar_1%3a5.5.8-1_amd64.deb ...\n",
            "Unpacking unrar (1:5.5.8-1) ...\n",
            "Setting up unrar (1:5.5.8-1) ...\n",
            "update-alternatives: using /usr/bin/unrar-nonfree to provide /usr/bin/unrar (unrar) in auto mode\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/unrar.1.gz because associated file /usr/share/man/man1/unrar-nonfree.1.gz (of link group unrar) doesn't exist\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Manually_Annotated.part06.rar\n",
            "\n",
            "WARNING: You need to start extraction from a previous volume to unpack Manually_Annotated_Images/459/b17b3c51cd8ae436d79e4939d4abcbfbccda8f584758c654bf4804bd.jpg\n",
            "\n",
            "Would you like to replace the existing file Manually_Annotated_Images/459/9e46cca055c628b4e4149303900d9bfc67041f9a02d9fab10a8debd4.jpg\n",
            "153782 bytes, modified on 2017-06-26 02:12\n",
            "with a new one\n",
            "153782 bytes, modified on 2017-06-26 02:12\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit N\n",
            "\n",
            "\n",
            "Would you like to replace the existing file Manually_Annotated_Images/459/b389c184d149db56bd584149a1661cc624bcd5492bcbded5a65d3858.jpg\n",
            "1551264 bytes, modified on 2017-06-26 02:10\n",
            "with a new one\n",
            "1551264 bytes, modified on 2017-06-26 02:10\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit E\n",
            "E\n",
            "\n",
            "Cannot find volume Manually_Annotated.part07.rar\n",
            "No files to extract\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u5fJTES24VHk",
        "colab_type": "code",
        "outputId": "ae8d860e-e726-49b0-fff9-cae9a157fbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "extract_file()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c78d8a07120f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_file' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "g859Z93j4c_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}